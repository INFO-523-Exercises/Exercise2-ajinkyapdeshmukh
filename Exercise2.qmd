---
title: "Exercise2"
author: "Ajinkya Deshmukh"
format: html
editor: visual
---

## Data Preprocessing in R

### Goal of the Exercise

-   Practicing basic R commands/methods for descriptive data analysis.

## Installing required packages

```{r}
# Installing package
if (!require(pacman))
  install.packages("pacman")
```

```{r}
library(pacman)

p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr) 
```

### Loading data

```{r}
#CSV files
data <- read_csv("data/x.csv")
data |> glimpse()
```

```{r}
#Tab separated files
data <- read_delim("data/x.tsv")
data |> glimpse()
```

### Importing data for MySQL database

```{r}
#Connecting database to MySQL database management system
#drv <- dbDriver("MySQL") #obtain the driver for MySQL, drivers available for other DBMS
```

### Using dplyr instead

```{r}
if (!require(dbplyr))
  install.packages("dbplyr") #install but donâ€™t run library() on this dbplyr.
```

```{r}
#con <- src_mysql("etcsite_charaparser", user = "termsuser", password = "termspassword", host = "localhost")
```

```{r}
#Commenting the below as connection to server is not available
#allwords <- tbl(con, "1_allwords")
#allwords
```

### Data Cleaning

### Wide vs. long format

```{r}
#Read data in wide format
wide <- read_delim(("data/wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

```{r}
#Long Format
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

#### Long to wide, use spread()

```{r}
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

### Split a column into multiple columns

```{r}
clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")
clean
```

### Handling date/time and time zones

```{r}
if (!require(lubridate))
  install.packages("lubridate")
  library(lubridate)
```

```{r}
#Convert dates of variance formats into one format:
mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day format
clean.dates
```

```{r}
#Extract day, week, month, year info from dates:
data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

```{r}
#Time zone:
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

```{r}
#Convert to Phoenix, AZ time:
with_tz(date.time, tz="America/Phoenix")
```

```{r}
#Change the timezone for a time:
force_tz(date.time, "Turkey")
```

```{r}
#Check available time zones:
OlsonNames()
```

### String Processing

-   Using the functions provided by **stringr** package to put column names back to a dataset.

```{r}
library(dplyr)
library(stringr)
library(readr)
```

```{r}
uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"
dataset <- "audiology/audiology.standardized"
```

-   **str_c:** string concatenation:

```{r}
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

-   Read the data file:

```{r}
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

```{r}
dim(data)
```

-   Read the name file line by line, put the lines in a vector:

```{r}
lines <- read_lines(url(namesF))
lines |> head()
```

-   Examine the content of lines and see the column names start on line 67, ends on line 135. Then, get column name lines and clean up to get column names:

    ```{r}
    names <- lines[67:135]
    names
    ```

```{=html}
<!-- -->
```
-   Observing a name line consisting of two parts, name: valid values.

    ```{r}
    names <- str_split_fixed(names, ":", 2) #split on regular expression pattern ":", this function returns a matrix
    names
    ```

-   Taking the first column, which contains names:

    ```{r}
    names <- names[,1]
    names
    ```

-   Cleaning up the names: trim spaces, remove ():

    ```{r}
    names <-str_trim(names) |> str_replace_all("\\(|\\)", "") # we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.
    names
    ```

-   Putting the columns to the data:

    ```{r}
    colnames(data)[1:69] <- names
    data
    ```

-   Rename the last two columns:

    ```{r}
    colnames(data)[70:71] <- c("id", "class")
    data
    ```

### Dealing with unknown values

-   Remove observations or columns with many NAs:

    ```{r}
    library(dplyr)
    missing.value.rows <- data |>
      filter(!complete.cases(data))
    missing.value.rows
    ```

    ```{r}
    data <- data %>%
      mutate(na_count = rowSums(is.na(data)))
    data
    ```

    ```{r}
    if (!require(tidyr))
      install.packages("tidyr")
      library(tidyr)
    data |>
      summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
      pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
      arrange(na_count)
    ```

```{=html}
<!-- -->
```
-   **bser** variable has 196 NAs. If this variable is considered not useful, given some domain knowledge, we can remove it from the data. From View, I can see bser in the 8th column:

    ```{r}
    data.bser.removed <- data %>%
      select(-8) %>%
      summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
    data.bser.removed
    ```

-   **matches** function can also help you find the index of a **colname** given its name:

    ```{r}
    data <- data %>%
      select(-matches("bser"))
    ```

### Mistaken characters

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

### Filling unknowns with most frequent values

```{r}
if (!require(DMwR2))
  install.packages("DMwR2")
  library(DMwR2)
data(algae, package = "DMwR2")
algae[48,]
```

-   **mxPH** is unknown.

    ```{r}
    # plot a QQ plot of mxPH
    if (!require("car"))
      install.packages("car")
      library(car)
    if (!require("ggplot2"))
      install.packages("ggplot2")
      library(ggplot2)
    ggplot(algae, aes(sample = mxPH)) +
      geom_qq_band() +
      stat_qq_point() +
        stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
      ggtitle("Normal QQ plot of mxPH")
    ```

-   The straight line fits the data pretty well so **mxPH** is normal, use mean to fill the unknown.

    ```{r}
    library(dplyr)
    algae <- algae |>
      mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
    algae
    ```

```{r}
if (!require("ggplot2"))
  install.packages("ggplot2")
  library(ggplot2)
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla")
```

```{r}
median(algae$Chla, na.rm = TRUE)
```

```{r}
mean(algae$Chla, na.rm = TRUE)
```

```{r}
algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

### Filling unknowns using linear regression

-   This method is used when two variables are highly correlated.

-   One value of variable A can be used to predict the value for variable B using the linear regression model.

    ```{r warning=FALSE}
    if (!require("tidyr"))
      install.packages("tidyr")
      library(tidyr)
    if (!require("corrr"))
      install.packages("corrr")
      library(corrr)
    algae_numeric <- algae[, 4:18] %>%
      drop_na()  # Removes rows with NA values

    cor_matrix <- algae_numeric |> correlate() |>  plot()
    ```

    ```{r}
    cor_matrix
    ```

-   Finding a linear model between **P04** and **oP04:**

    ```{r}
    algae <- algae %>%
      filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. 

    m = lm(PO4 ~ oPO4, data = algae)
    lm(formula = PO4 ~ oPO4, data = algae)
    ```

-   Check if the model is good

    ```{r}
    m |> 
      summary()
    ```

-   The **F-test of overall significance** determines whether this relationship is statistically significant.

-   This **lm** is **P04 = 1.293\*oP04 + 42.897**

    ```{r}
    algae$PO4
    ```

    ```{r}
    algae <- algae %>%
      mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
    ```

    ```{r}
    res = resid(m)

    oPO4_reduced <- algae %>%
      filter(row_number() != 28) %>%
      pull(oPO4)
    ```

    ```{r}
    ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
      geom_point() +
      geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
      labs(
        x = "oPO4",
        y = "residuals",
        title = "Residual Plot"
      )
    ```

-   **sapply()** can be used to apply the transformation to a set of values

-   Create a simple function **fillP04:**

    ```{r}
    fillPO4 <- function(x) {
      if_else(is.na(x), 42.897 + 1.293 * x, x)
    }
    #if x is not NA, return 42.897+1.293*x 
    ```

    ```{r}
    algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
    ```

### Filling unknowns by exploring similarities among cases

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

-   **DM2R2** provides a method call **knnImputation().** This method uses the Euclidean distance to find the ten most similar cases of any water sample with some unknown value in a variable, and then use their values to fill in the unknown.

    ```{r}
    algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


    data(algae, package="DMwR2") #get data again so there are unknown values
    algae <- algae[-manyNAs(algae), ] 
    algae <- knnImputation(algae, k = 10, meth="median") #use the median of k most similar samples
    ```

    ```{r}
    getAnywhere(knnImputation())
    ```

### Scaling and normalization

-   Normalize value **x:** y = (x - mean) / standard deviation(x) using **scale()**

    ```{r}
    library(dplyr)
    library(palmerpenguins)
    ```

    ```{r}
    data(penguins)
    ```

    ```{r}
    # select only numeric columns
    penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

    # normalize numeric columns
    penguins_norm <- scale(penguins_numeric)

    # convert back to data frame and add species column
    peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

    # because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.
    ```

    ```{r}
    summary(penguins)
    ```

    ```{r}
    summary(peng.norm)
    ```

-   **scale()** can also take an argument for center and an argument of scale to normalize data in some other ways, E.g. y = (x -min) / (max - min)

    ```{r}
    max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
    min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
    ```

    ```{r}
    max
    ```

    ```{r}
    min
    ```

    ```{r}
    # min-max normalization
    penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

    penguin_scaled <- cbind(penguins_norm, species = penguins$species)

    summary(penguin_scaled)
    ```

### Discretizing variables (binning)

-   Process of transferring continuous functions, models, variables, and equations into discrete counterparts

    ```{r}
    data(Boston, package="MASS")
    summary(Boston$age)
    ```

    ```{r}
    Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") #create 5 bins and add new column newAge to Boston
    summary(Boston$newAge)
    ```

    ```{r}
    Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") #add labels

    summary(Boston$newAge)
    ```

### Equal-depth

```{r}
if(!require("Hmisc"))
  install.packages("Hmisc")
  library(Hmisc)
Boston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins and add new column newAge to Boston

table(Boston$newAge)
```

### Assign labels

```{r}
Boston$newAge <- factor(cut2(Boston$age, g = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

```{r}
hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 â€“ 101.
```

```{r}
library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

### Decimal scaling

```{r}
data <- c(10, 20, 30, 50, 100)
```

```{r}
(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters
```

```{r}
(decimalScale = data / (10^nDigits))
```

### Smoothing by bin mean

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

```{r}
#Finding average of each bin:
(bin_means = apply(bins, 1, FUN = mean))
```

```{r}
#Replace values with their bin mean:
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

### Variable correlations and dimensionality reduction

#### Chi-squared test

```{r}
#data (contingency table):
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

### Loglinear model

-   Extending chi-squared to more than 2 categorical variables.

-   Loglinear models model cell counts in contingency tables.

```{r}
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
seniors
```

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

-   Use **\*** to connect all variables to get a saturated model, which will fit the data perfectly. Then we will remove effects that are not significant.

    ```{r}
    mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
    summary(mod.S4)
    ```

-   Remove age and re-generate a model with the remaining three variables.

    ```{r}
    mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
    summary(mod.S3)
    ```

-   For data modelling, we can remove the 3-way interaction by testing **"Freq \~ (cigarette + marijuana + alcohol)\^2"**

    ```{r}
    mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
    summary(mod.3)
    ```

    ```{r}
    cbind(mod.3$data, fitted(mod.3))
    ```

### Correlations

```{r}
library(tidyr) # data manipulation
penguins_numeric |> 
  drop_na() |>
  correlate()
```

### Principal components analysis (PCA)

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
head(pca$scores) # pca result is a list, and the component scores are elements in the list
```

-   **comp3 = 0.94\*bill_length_mm + 0.144\*''bill_depth_mm''-0.309\*flipper_length_mm**

    ```{r}
    penguins_na <- penguins |> 
      drop_na()

    peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

    head(peng.reduced)
    ```

```{r}
if(!require("wavelets"))
  install.packages("wavelets.tar.gz", repos = NULL, type = "source")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3) #with 8-element vector, 3 level is the max.
wt
```

-   Reconstructing the original:

    ```{r}
    idwt(wt)
    ```

    ```{r}
    xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
    xt
    ```

    ```{r}
    idwt(xt)
    ```

### Sampling

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

### Simple random sampling, without replacement:

```{r}
sample(age, 5)
```

### Simple random sampling, with replacement:

```{r}
sample(age, 5, replace = TRUE)
```

### Stratified sampling

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

### Cluster sampling

```{r warning=FALSE}
if(!require("sampling"))
  install.packages("sampling", type = "source")
library(sampling)
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

### Handling Text Datasets

```{r warning=FALSE}
pacman::p_load(tm,SnowballC)#tm uses SnowballC for stemming
# read corpus
# Emails.csv,  holding some of Hillary's emails
data <- read.csv("data/Emails.csv")

docs <- Corpus(VectorSource(data$RawText))
mode(docs)
```

#### Inspect a document

```{r}
docs[[20]]
```

## **Preprocessing text**

```{r warning=FALSE}
docs <- docs |>
         tm_map(removePunctuation) |>
         tm_map(content_transformer(tolower)) |> #to lower case
         tm_map(removeNumbers) |>
         tm_map(removeWords, stopwords("en")) |> #stopwords, such as a, an.
         tm_map(stripWhitespace) |>
         tm_map(stemDocument) #e.g. computer -> comput
```

```{r}
content(docs[[20]]) #note: stemming reduces a word to its â€˜rootâ€™ with the aassumption that the â€˜rootâ€™ represents the semantics of a word, e.g. computer, computing, computation, computers are about the concept of compute, which may be represented by â€˜computâ€™. but stemming is never perfect.
```

```{r warning=FALSE}
DTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
DTData
```

```{r}
inspect(DTData[1:2, 1:5])
```

```{r warning=FALSE}
TDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
inspect(TDData[1:2, 1:5])
```

### Explore the dataset

```{r}
findFreqTerms(TDData, lowfreq = 75, highfreq = 1000)
```

```{r}
#Finding correlations among terms:
findAssocs(TDData, terms = "bill", corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("bill"), corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("schedul"), corlimit = 0.3)
```

### Create a word cloud

```{r}
if(!require("wordcloud"))
  install.packages("wordcloud")
if(!require("RColorBrewer"))
install.packages("RColorBrewer")
library(wordcloud)
```

```{r}
#Loading a required package: RColorBrewer
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

-   **png()** opens a new device 'png' to output the graph to a local file:

    ```{r}
    png(file = "wordCloud.png", width = 1000, height = 700, bg= "grey30")

    wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
    random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
    dev.off() #closing png device
    ```

-   Output the graph to the default display in RStudio

    ```{r}
    #Commenting the below code as the required file is not found at the specified directory
    wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
    random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
    ```

    ```{r warning=FALSE}
    if(!require("onehot"))
      install.packages("onehot")
    library(onehot)
    d <- data.frame(language=c("javascript", "python", "java"), hours=c(10, 3, 5) )
    d$language <- as.factor(d$language) #convert the column to be encoded to Factor
    encoded <- onehot(d)
    new_d <- predict(encoded, d)
    new_d
    ```

    ```{r warning=FALSE}
    if(!require("qdapTools"))
      install.packages("qdapTools")
    library(qdapTools)
    d <- data.frame(language=c("javascript, python", "java"), hours = c(3, 5) )
    d
    ```

    ```{r}
    dlist <- as.list(d)
    new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

    new_d
    ```

## ADVANCED

Exercises on your data set:

1.  What attributes are there in your data set?

    ```{r}
    pacman::p_load(formattable)

    # Let's load a data set from the x data set
    episode <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-24/episodes.csv') 

    episode  |>
      head() |>
      formattable()
    #Showing the attributes in the dataset
    str(episode)
    ```

2.  Do you have highly correlated attributes? How did you find out about the correlations or lack of correlations?

    -\> Yes, the dataset contains highly correlated attributes as plotted below. Correaltion and lack of correlations can be found by plotting the correlation between numerical attributes.

    ```{r warning=FALSE}
    episodes_Num <- select_if(episode, is.numeric)
    correlation_matrix <- cor(episodes_Num)

    #Printing correlation matrix
    episodes_Num |>
    plot_correlate()
    ```

3.  Do you have numerical attributes that you might want to bin? Try at least two methods and compare the differences.

    -\> Yes, there are numerical attributes in the dataset and binning can be used on the dataset.

    -\> Binning type equal to divide the episodes based on IMDB ratings and classify them from bad to best rated episodes.

    ```{r}
    episodes_Num$new_ratings <- dlookr::binning(episodes_Num$n_ratings, 5, type = "equal")
    summary(episodes_Num$new_ratings)
    ```

    ```{r}
    episodes_Num$new_ratings <- dlookr::binning(episodes_Num$n_ratings, nbins = 5, labels = c("Bad","Fair","Average","Good","Excellent"), type = "equal") #added labels
    summary(episodes_Num$new_ratings)
    ```

    -\> Another way to perform binning is by using cut() function, it helps for binning data into different intervals.

    ```{r}
    custom_breaks <- c(0, 30, 60, 90, 120, 150)
    bins <- cut(episodes_Num$n_ratings, breaks = custom_breaks)
    print(bins)
    ```

4.  If you have categorical attributes, use the concept hierarchy generation heuristics (based on attribute value counts) suggested in the textbook to produce some concept hierarchies. How well does this approach work for your attributes?

    ```{r warning=FALSE}
    episodes_NA <- na.omit(episodes_Num)
    hc <- hclust(dist(episodes_NA), method = "complete")
    num_clusters <- 3
    clusters <- cutree(hc, k=num_clusters)
    print(clusters)
    ```
